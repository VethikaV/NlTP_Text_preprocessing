{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ3GMyrxrhu96p9GBMrMzH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VethikaV/STEP_CDW_Intern/blob/Day-6/Poem_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PYTHON"
      ],
      "metadata": {
        "id": "GiPc_lARghXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoemGenerator:\n",
        "    def __init__(self):\n",
        "        self.greeting = \"I am a poem generator assistant, expert in generating poem in Shakespearean terms. Please ask me a poem related query.\"\n",
        "\n",
        "    def generate_poem(self, topic, lines=4):\n",
        "        if topic:\n",
        "            poem = \"\"\n",
        "            for i in range(lines):\n",
        "                if i % 2 == 0:\n",
        "                    poem += f\"Oh, {topic}! Thou art so {self.get_adjective()},\" + \"\\n\"\n",
        "                else:\n",
        "                    poem += f\"Thy {self.get_noun()} doth make my heart {self.get_verb()},\" + \"\\n\"\n",
        "            return poem\n",
        "        else:\n",
        "            return self.greeting\n",
        "\n",
        "    def get_adjective(self):\n",
        "        adjectives = [\"fair\", \"lovely\", \"kind\", \"gracious\", \"beautiful\", \"charming\", \"delightful\", \"wonderful\"]\n",
        "        return self.random_choice(adjectives)\n",
        "\n",
        "    def get_noun(self):\n",
        "        nouns = [\"light\", \"smile\", \"touch\", \"grace\", \"face\", \"voice\", \"sight\", \"charm\"]\n",
        "        return self.random_choice(nouns)\n",
        "\n",
        "    def get_verb(self):\n",
        "        verbs = [\"sing\", \"dance\", \"laugh\", \"shine\", \"inspire\", \"ignite\", \"delight\", \"enchant\"]\n",
        "        return self.random_choice(verbs)\n",
        "\n",
        "    def random_choice(self, lst):\n",
        "        import random\n",
        "        return random.choice(lst)\n",
        "\n",
        "# Initialize the PoemGenerator object\n",
        "poem_generator = PoemGenerator()\n",
        "\n",
        "# Get the topic and number of lines from the user\n",
        "topic = input(\"Enter the topic for the poem: \")\n",
        "lines = int(input(\"Enter the number of lines: \"))\n",
        "\n",
        "# Generate the poem\n",
        "poem = poem_generator.generate_poem(topic, lines)\n",
        "print(poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx9lspWGH13B",
        "outputId": "8920e987-fdce-4858-e012-14077f719735"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic for the poem: nature\n",
            "Enter the number of lines: 5\n",
            "Oh, nature! Thou art so lovely,\n",
            "Thy face doth make my heart enchant,\n",
            "Oh, nature! Thou art so fair,\n",
            "Thy touch doth make my heart delight,\n",
            "Oh, nature! Thou art so beautiful,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IEJQdANrWtp8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGMLS28IdebN",
        "outputId": "96bbecc5-1366-429b-97ea-0948a69a2d18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'object': 'list', 'data': [{'id': 'llama-3.2-11b-vision-preview', 'object': 'model', 'created': 1727226869, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.1-8b-instant', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None}, {'id': 'llama3-70b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'distil-whisper-large-v3-en', 'object': 'model', 'created': 1693721698, 'owned_by': 'Hugging Face', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'llama3-8b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.3-70b-specdec', 'object': 'model', 'created': 1733505017, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'whisper-large-v3', 'object': 'model', 'created': 1693721698, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'llama-3.2-90b-vision-preview', 'object': 'model', 'created': 1727226914, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'mixtral-8x7b-32768', 'object': 'model', 'created': 1693721698, 'owned_by': 'Mistral AI', 'active': True, 'context_window': 32768, 'public_apps': None}, {'id': 'llama-3.3-70b-versatile', 'object': 'model', 'created': 1733447754, 'owned_by': 'Meta', 'active': True, 'context_window': 32768, 'public_apps': None}, {'id': 'whisper-large-v3-turbo', 'object': 'model', 'created': 1728413088, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'llama-3.2-1b-preview', 'object': 'model', 'created': 1727224268, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.2-3b-preview', 'object': 'model', 'created': 1727224290, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'deepseek-r1-distill-llama-70b', 'object': 'model', 'created': 1737924940, 'owned_by': 'DeepSeek / Meta', 'active': True, 'context_window': 131072, 'public_apps': None}, {'id': 'gemma2-9b-it', 'object': 'model', 'created': 1693721698, 'owned_by': 'Google', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-guard-3-8b', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "  \"model\": \"mixtral-8x7b-32768\",\n",
        "  \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"how to start with LLM\"\n",
        "      }\n",
        "  ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "print(response.json()['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojp2nWXteAvm",
        "outputId": "f12739a9-00b2-4168-b9e5-e4faa6379708"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An LLM (Master of Laws) is an advanced degree in law that typically requires a first degree in law (such as a JD in the United States) as a prerequisite. To start an LLM program, you should first research and choose a school that offers the program and specialization that interests you. You will then need to apply to the school, typically by submitting transcripts, letters of recommendation, and a personal statement. Once accepted, you will need to register for classes and complete the required coursework and any thesis or research project to earn the degree. It can be helpful to speak with an academic advisor or career counselor to determine if an LLM is the right fit for your goals and to plan your course of study.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING GROQ"
      ],
      "metadata": {
        "id": "CF9jvmy2fUXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXJchIuOeXYg",
        "outputId": "97655c38-db3e-49b8-9455-ce69ba88d71c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"act as a docotr, i am a patient...i have headache what can it be\",#message\n",
        "        }\n",
        "    ],\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        ")"
      ],
      "metadata": {
        "id": "1vc7MuDzeXm-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content)\n",
        "# print(chat_completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZP1piHBfww5",
        "outputId": "d45d42ac-fd70-46b7-9953-d605ddc09943"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not a doctor, but I can share some general information that might be helpful. A headache can be caused by many things, and it's important to identify the specific cause in order to determine the best treatment. Here are some common causes of headaches:\n",
            "\n",
            "1. Tension headaches: These are the most common type of headaches and are often caused by muscle tension in the neck and scalp.\n",
            "2. Migraines: These are severe headaches that often cause throbbing pain on one side of the head. Migraines can also cause nausea, vomiting, and sensitivity to light and sound.\n",
            "3. Sinus headaches: These are caused by inflammation in the sinuses, which are located in the forehead, cheeks, and bridge of the nose.\n",
            "4. Cluster headaches: These are severe headaches that occur in clusters, meaning they occur several times a day for several weeks or months.\n",
            "5. Rebound headaches: These are headaches that occur as a result of overuse of pain medication.\n",
            "\n",
            "It's important to speak with a healthcare professional to determine the cause of your headaches and to develop a treatment plan. If your headache is severe or accompanied by other symptoms, such as vomiting, vision changes, or difficulty speaking, seek medical attention immediately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using Langchain"
      ],
      "metadata": {
        "id": "r5CPBeFlhW2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "id": "8EMt1D30hOFO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "6a9cF44Khdxt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"hi\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzGQXCtZhfLW",
        "outputId": "46980765-3eff-4005-92a2-83899fd828bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today? If you have any questions or need assistance with\n",
            "\n",
            "* general knowledge\n",
            "* programming\n",
            "* data analysis\n",
            "* machine learning\n",
            "* deep learning\n",
            "* natural language processing\n",
            "* software engineering\n",
            "\n",
            "feel free to ask! I'll do my best to provide a clear and helpful answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POEM GENERATOR"
      ],
      "metadata": {
        "id": "MvlqYEhJiTrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem = llm.invoke(\"Generate a 8 line poem\")\n",
        "print(poem.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukVZT5YLiVat",
        "outputId": "9208997d-55b7-481c-e350-db534931fb9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underneath the sapphire sky,\n",
            "Where the sun's warm rays do lie,\n",
            "Lies a world of endless beauty,\n",
            "Where dreams and wonders never die.\n",
            "\n",
            "The flowers bloom with colors bright,\n",
            "As the birds take flight in the light,\n",
            "In this place of pure delight,\n",
            "Life's true essence takes its flight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ( \"system\", '''\n",
        "                  You are a dedicated poem generator assistant, specialized in crafting poems in Shakespearean style. Your task is strictly to generate poems based on the topic and number of lines provided by the user. Follow these guidelines:\n",
        "\n",
        "                  1. Only respond to queries explicitly requesting a poem on a specific topic.\n",
        "                  2. The output must strictly be the poem itself, formatted in Shakespearean terms, with no additional explanations, descriptions, or headers.\n",
        "                  3. If the query is unrelated to poem generation (e.g., generating code, recipes, suggestions, general knowledge questions, or any other non-poetry tasks), respond with:\n",
        "                    \"I am a poem generator assistant, expert in generating poems in Shakespearean terms. Please ask me a poem-related query.\"\n",
        "                  4. Do not perform any tasks beyond poem generation. Always fall back to the above message for non-poetry-related queries.\n",
        "\n",
        "                  Note: The assistant must ensure the generated poem aligns with the requested topic and the specified number of lines. If the number of lines is not specified, default to 14 lines (a Shakespearean sonnet).\n",
        "                '''\n",
        "    ),\n",
        "    ( \"human\", \"write a 8 line poem\" ),\n",
        "    # ( \"human\", \"write a code for addition\" ),\n",
        "]\n",
        "\n",
        "# llm.invoke(messages)\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5pMGcgxLLRd",
        "outputId": "1ec60eab-fb66-494e-8304-707d52b36ff3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upon the eve, the stars in twinkling dance,\n",
            "Their radiant glow, a silent, sweet romance.\n",
            "In quiet whispers, trees their secrets share,\n",
            "A gentle breeze, the only witness there.\n",
            "\n",
            "A waning moon, her silver light bestows,\n",
            "Upon the world, a tranquil, soft repose.\n",
            "In nature's arms, the night's sweet, soft allure,\n",
            "A peaceful slumber, pure and sure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaining"
      ],
      "metadata": {
        "id": "P_VUcKEUM4SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ( \"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\", ),\n",
        "        ( \"human\", \"{input}\" ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWbz0AF0M5gT",
        "outputId": "dd5a2564-03a0-4390-8382-ce4d41ce7b5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='That\\'s great! I can definitely help you translate English to German. Here\\'s the translation for your sentence:\\n\\n\"Ich liebe Programmieren.\"\\n\\nLet me know if you need any further assistance!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 23, 'total_tokens': 71, 'completion_time': 0.072771482, 'prompt_time': 0.003191593, 'queue_time': 0.035478096, 'total_time': 0.075963075}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-f0877b1d-7435-4dea-924c-34fd172a81b6-0', usage_metadata={'input_tokens': 23, 'output_tokens': 48, 'total_tokens': 71})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}